{
  "cells": [
    {
      "metadata": {
        "_uuid": "78b6b671013de316d0f069feefc7ff889e6451e5",
        "id": "XYxOtbOArzrK"
      },
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ]
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "_kg_hide-input": true,
        "id": "JobEGrQHrzrL"
      },
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats.stats import pearsonr\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "%matplotlib inline\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6f4f3f688a851ac4c8875a8b5edf54fcd731284e",
        "id": "tvJEPobirzrM"
      },
      "cell_type": "markdown",
      "source": [
        "### Auxiliary functions"
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "d6f043ecd7bbfb5c874aed6f128b915d3ffdb0a0",
        "id": "LeuNBpPArzrM"
      },
      "cell_type": "code",
      "source": [
        "def cross_validate(estimator, train, validation):\n",
        "    X_train = train[0]\n",
        "    Y_train = train[1]\n",
        "    X_val = validation[0]\n",
        "    Y_val = validation[1]\n",
        "    train_predictions = classifier.predict(X_train)\n",
        "    train_accuracy = accuracy_score(train_predictions, Y_train)\n",
        "    train_recall = recall_score(train_predictions, Y_train)\n",
        "    train_precision = precision_score(train_predictions, Y_train)\n",
        "\n",
        "    val_predictions = classifier.predict(X_val)\n",
        "    val_accuracy = accuracy_score(val_predictions, Y_val)\n",
        "    val_recall = recall_score(val_predictions, Y_val)\n",
        "    val_precision = precision_score(val_predictions, Y_val)\n",
        "\n",
        "    print('Model metrics')\n",
        "    print('Accuracy  Train: %.2f, Validation: %.2f' % (train_accuracy, val_accuracy))\n",
        "    print('Recall    Train: %.2f, Validation: %.2f' % (train_recall, val_recall))\n",
        "    print('Precision Train: %.2f, Validation: %.2f' % (train_precision, val_precision))\n",
        "\n",
        "    return train_accuracy, train_recall, train_precision, val_accuracy, val_recall, val_precision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69863afcf2938c019b9025f4aa7dcafdad7bae03",
        "id": "fcqeMHbWrzrN"
      },
      "cell_type": "markdown",
      "source": [
        "### Load data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "58307dcedd682f2353681fb1fb724aa059271605",
        "_kg_hide-output": false,
        "id": "cy27SlJkrzrN"
      },
      "cell_type": "code",
      "source": [
        "train_raw = pd.read_csv('/content/Taitanic.csv')\n",
        "test_raw = pd.read_csv('/content/Taitanic.csv')\n",
        "test_ids = test_raw['PassengerId'].values\n",
        "\n",
        "# Join data to analyse and process the set as one.\n",
        "train_raw['train'] = 1\n",
        "test_raw['train'] = 0\n",
        "data = pd.concat([train_raw, test_raw], ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "fa61280471a2e54b91d63c03ba42e6d20e7ca55b",
        "id": "IPhRuqrTrzrN"
      },
      "cell_type": "markdown",
      "source": [
        "### Overview the data"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17e2647441b6365242283a3ed718fe355d10429b",
        "_kg_hide-input": true,
        "id": "zFIO778VrzrO",
        "collapsed": true
      },
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "020c32baeb833a88b486e9151da3305eca9354fd",
        "_kg_hide-input": true,
        "id": "cnBovYZfrzrO"
      },
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bffc61a690870527eb34aa8c19ef31928358476b",
        "id": "QmEU1roUrzrO"
      },
      "cell_type": "markdown",
      "source": [
        "One advantage of Bayesian models is that it works well enough with small data, having more would give you more accurate probabilities but it's not data hungry as something like deep learning.\n",
        "\n",
        "### Pre-process\n",
        "* feature selection, data cleaning, feature engineering and data imputation"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "acbdfcabff5a80d326615e42629f3bbd8c84edbe",
        "id": "P-SiBIsbrzrO"
      },
      "cell_type": "code",
      "source": [
        "features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp']\n",
        "target = 'Survived'\n",
        "\n",
        "data = data[features + [target] + ['train']]\n",
        "# Categorical values need to be transformed into numeric.\n",
        "data['Sex'] = data['Sex'].replace([\"female\", \"male\"], [0, 1])\n",
        "data['Embarked'] = data['Embarked'].replace(['S', 'C', 'Q'], [1, 2, 3])\n",
        "data['Age'] = pd.qcut(data['Age'], 10, labels=False, duplicates='drop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96b9f5730cd6c5e21e96567252b98ff81eb5ab54",
        "_kg_hide-output": false,
        "id": "I61uCsG5rzrO"
      },
      "cell_type": "code",
      "source": [
        "# Split data into train and test.\n",
        "train = data.query('train == 1')\n",
        "test = data.query('train == 0')\n",
        "\n",
        "# Drop missing values from the train set.\n",
        "train.dropna(axis=0, inplace=True)\n",
        "labels = train[target].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3ab1d08c7de6c5c6d939c15e43066da44589950b",
        "id": "GCI1Xb-hrzrO"
      },
      "cell_type": "markdown",
      "source": [
        "Our processed train set"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0701969fc0bbe441860961dfe53326e3e4430897",
        "_kg_hide-input": true,
        "id": "aCG6vwxxrzrP"
      },
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9f1ee0bb0675876d1b3b13c7aeb966228b92160",
        "id": "OD7XLxx4rzrP"
      },
      "cell_type": "markdown",
      "source": [
        "### Correlation study\n",
        "* As we saw Naive Bayes models expect the features to be independent, so let's apply the Pearson correlation coefficient on them to give us a hint about how independent they are from the others."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0ab48f4472c0fb34692abac3880404207989d7e",
        "_kg_hide-input": true,
        "id": "OHVE0zBLrzrP"
      },
      "cell_type": "code",
      "source": [
        "columns = train[features + [target]].columns.tolist()\n",
        "nColumns = len(columns)\n",
        "result = pd.DataFrame(np.zeros((nColumns, nColumns)), columns=columns)\n",
        "\n",
        "# Apply Pearson correlation on each pair of features.\n",
        "for col_a in range(nColumns):\n",
        "    for col_b in range(nColumns):\n",
        "        result.iloc[[col_a], [col_b]] = pearsonr(train.loc[:, columns[col_a]], train.loc[:,  columns[col_b]])[0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10,10))\n",
        "ax = sns.heatmap(result, yticklabels=columns, vmin=-1, vmax=1, annot=True, fmt='.2f', linewidths=.2)\n",
        "ax.set_title('PCC - Pearson correlation coefficient')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "047a8fb7db2b58efdaa61c7fe11dd86f2ffa2187",
        "id": "vtzAtmHfrzrP"
      },
      "cell_type": "markdown",
      "source": [
        "About the correlation between the features, we can see that \"Fare\" and \"Pclass\" seem to be highly related, so i'll remove \"Pclass\". Also features like \"Sex\", \"Pclass\" and \"Fare\" should be good predictors.\n",
        "\n",
        "### Distribution study\n",
        "* Also the model expect the features to come from a Gaussian (or normal) distribution, so let's check that as well."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfbe5de3e2ab921f7df8752e16756d63e7ec0047",
        "_kg_hide-input": true,
        "id": "Fp05jJxErzrP"
      },
      "cell_type": "code",
      "source": [
        "continuous_numeric_features = ['Age', 'Fare', 'Parch', 'SibSp']\n",
        "for feature in continuous_numeric_features:\n",
        "    sns.distplot(train[feature])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "2d9cb0c7c06827d7324b153426c955a68e60294e",
        "id": "cvTN15QqrzrP"
      },
      "cell_type": "markdown",
      "source": [
        "Looking at our continuous numeric features we can see that \"Fare\", \"Parch\" and \"SibSp\", have a distribution close to normal, but with a left side skew, \"Age\" have a distribution a a bit different from the other but maybe it's close enough to Gaussian."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bfb14dfc915c90e09870f4ca14c0b3f8806e7446",
        "_kg_hide-output": false,
        "id": "sqvY3SKDrzrP"
      },
      "cell_type": "code",
      "source": [
        "train.drop(['train', target, 'Pclass'], axis=1, inplace=True)\n",
        "test.drop(['train', target, 'Pclass'], axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_uuid": "92235de1df681e329f1e70527abca5cfdb9cdb4d",
        "id": "zpshtJPWrzrP"
      },
      "cell_type": "markdown",
      "source": [
        "### Split data in train and validation (80% ~ 20%)"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2c7b37d4af8a07a1bc448fff1aa9252c398d88b1",
        "id": "j0Ef_c1nrzrP"
      },
      "cell_type": "code",
      "source": [
        "X_train, X_val, Y_train, Y_val = train_test_split(train, labels, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "96ee31cb80df049d40daa7708450f3499e0f3204",
        "_kg_hide-input": true,
        "id": "uZIM8LR2rzrQ"
      },
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "38ff354d30c45580677cd35d21aad7b015a6c3e1",
        "id": "F4owahDZrzrQ"
      },
      "cell_type": "markdown",
      "source": [
        "### Split train data into two parts"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c709e510191b63562407a2b23401c63508bad83",
        "id": "7vl8VesOrzrQ"
      },
      "cell_type": "code",
      "source": [
        "X_train1, X_train2, Y_train1, Y_train2 = train_test_split(X_train, Y_train, test_size=0.3, random_state=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ff22e878edf3d99d7ff272aa9653b36140505da1",
        "id": "OM8zSuGOrzrQ"
      },
      "cell_type": "code",
      "source": [
        "classifier = GaussianNB()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d62839bd20993df42c2b1fffd2ddddcb24da7731",
        "id": "GdS70Pa6rzrQ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Fit the first part\n",
        "* Fitting data here is really fast."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "cef67c99c01a6b83caa7b9861c2603dafa2619f2",
        "id": "rKP6en3QrzrQ"
      },
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train2, Y_train2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0c729862f56c1201042e4899700d7ee9196abb03",
        "_kg_hide-input": true,
        "id": "3otus1_RrzrQ"
      },
      "cell_type": "code",
      "source": [
        "print('Metrics with only 30% of train data')\n",
        "train_acc_30, train_rec_30, train_prec_30, val_acc_30, val_rec_30, val_prec_30 = cross_validate(classifier, (X_train, Y_train), (X_val, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "40f3ecb8b8d66be494d3d33a88a44658afefa46c",
        "id": "C5FpBjSErzrQ"
      },
      "cell_type": "markdown",
      "source": [
        "#### Update the model with the second part\n",
        "* Nice thing about this kind of model, you can update it by just fitting the model again with more data."
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dcad148d72770b44b05c2d3e34cc134ccda1b958",
        "id": "eUULPnzJrzrQ"
      },
      "cell_type": "code",
      "source": [
        "classifier.partial_fit(X_train1, Y_train1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "717d0760281c0f8ab51e22c6b880d0069c6a39ae",
        "_kg_hide-input": true,
        "id": "YUeSHVnyrzrQ"
      },
      "cell_type": "code",
      "source": [
        "print('Metrics with the remaining 70% of train data')\n",
        "train_acc_100, train_rec_100, train_prec_100, val_acc_100, val_rec_100, val_prec_100 = cross_validate(classifier, (X_train, Y_train), (X_val, Y_val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0126f37767fbe291f20a5ebf965353fedacfcf17",
        "id": "OBMofdKSrzrQ"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see our results improved after we updated  the model with the remaining data.\n",
        "\n",
        "The sklearn model also give us some interesting options from the model API about the target class."
      ]
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "8d6d05d83697544629765faf91211c85d1c7c839",
        "id": "UR68MQUVrzrQ"
      },
      "cell_type": "code",
      "source": [
        "print('Probability of each class')\n",
        "print('Survive = 0: %.2f' % classifier.class_prior_[0])\n",
        "print('Survive = 1: %.2f' % classifier.class_prior_[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "e89e0ee92e19686d25906bd677f73e8d88f304c5",
        "id": "LGneqk9yrzrQ"
      },
      "cell_type": "code",
      "source": [
        "print('Mean of each feature per class')\n",
        "print('               Age         Embarked   Fare         Parch       Sex         SibSp')\n",
        "print('Survive = 0: %s' % classifier.theta_[0])\n",
        "print('Survive = 1: %s' % classifier.theta_[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "_uuid": "b25337bf9fbb2433da6d385849dac53ac295bb4a",
        "id": "arMNfuHNrzrQ"
      },
      "cell_type": "code",
      "source": [
        "print('Variance of each feature per class')\n",
        "print('Survive = 0: %s' % classifier.var_[0])\n",
        "print('Survive = 1: %s' % classifier.var_[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "704dcce51b9a7d5052dba8c99fd75b6c1d7c5868",
        "id": "uWHq1AhXrzrR"
      },
      "cell_type": "markdown",
      "source": [
        "### Apply the model on the test data and create submission"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f53ad1e46f92415bd2a28ac157785f0baf631d4e",
        "id": "bKK9MsbhrzrZ"
      },
      "cell_type": "code",
      "source": [
        "# Unfortunately sklearn naive Bayes algorithm currently do not make inference with missing data (but should do), so we need to input missing data.\n",
        "test.fillna(test.mean(), inplace=True)\n",
        "test_predictions = classifier.predict(test)\n",
        "submission = pd.DataFrame({'PassengerId': test_ids})\n",
        "submission['Survived'] = test_predictions.astype('int')\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "submission.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e614ec75"
      },
      "source": [
        "metrics = ['Accuracy', 'Recall', 'Precision']\n",
        "\n",
        "# Data for 30% train\n",
        "train_30_data = [train_acc_30, train_rec_30, train_prec_30]\n",
        "val_30_data = [val_acc_30, val_rec_30, val_prec_30]\n",
        "\n",
        "# Data for 100% train\n",
        "train_100_data = [train_acc_100, train_rec_100, train_prec_100]\n",
        "val_100_data = [val_acc_100, val_rec_100, val_prec_100]\n",
        "\n",
        "width = 0.2\n",
        "r1 = np.arange(len(metrics))\n",
        "r2 = [x + width for x in r1]\n",
        "r3 = [x + width for x in r2]\n",
        "r4 = [x + width for x in r3]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "ax.bar(r1, train_30_data, width, label='Train (30% data)', color='skyblue')\n",
        "ax.bar(r2, val_30_data, width, label='Validation (30% data)', color='steelblue')\n",
        "ax.bar(r3, train_100_data, width, label='Train (100% data)', color='lightcoral')\n",
        "ax.bar(r4, val_100_data, width, label='Validation (100% data)', color='indianred')\n",
        "\n",
        "ax.set_xlabel('Metric', fontweight='bold')\n",
        "ax.set_ylabel('Score', fontweight='bold')\n",
        "ax.set_title('Model Performance: 30% vs 100% Training Data', fontweight='bold')\n",
        "ax.set_xticks([r + 1.5 * width for r in range(len(metrics))])\n",
        "ax.set_xticklabels(metrics)\n",
        "ax.legend()\n",
        "ax.set_ylim(0, 1)\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4bc43cf15a8e494f43034b6a1d8a1b42555d2c04",
        "id": "PnPOizpkrzrZ"
      },
      "cell_type": "markdown",
      "source": [
        "### What are the Pros and Cons of Naive Bayes?\n",
        "#### Pros:\n",
        "* Humans are not good with reasoning in systems with limited or conflicting information. It would be handy if we have something to manage all this limited/conflicting information.\n",
        "* It is easy and fast to predict class of test data set. It also perform well in multi class prediction\n",
        "* When assumption of independence holds, a Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data.\n",
        "* It perform well in case of categorical input variables compared to numerical variable(s). For numerical variable, normal distribution is assumed (bell curve, which is a strong assumption).\n",
        "\n",
        "#### Cons:\n",
        "* Probably the most notable weakness of BNs is the designing methodology.There is no standard way of building BNs.\n",
        "* If categorical variable has a category (in test data set), which was not observed in training data set, then model will assign a 0 (zero) probability and will be unable to make a prediction. This is often known as “Zero Frequency”. To solve this, we can use the smoothing technique. One of the simplest smoothing techniques is called Laplace estimation.\n",
        "* On the other side naive Bayes is also known as a bad estimator, so the probability outputs from predict_proba are not to be taken too seriously.\n",
        "* Another limitation of Naive Bayes is the assumption of independent predictors. In real life, it is almost impossible that we get a set of predictors which are completely independent.\n",
        "\n",
        "##### The design of a BN can be a considerable amount of effort in complex systems and it is based on the knowledge of the expert(s) who designed it. Although, this disadvantage can be good in another point of view, since BNs can be easily inspected by the designers and has the guarantee that the domain specific information is being used.\n",
        "\n",
        "### Tips to improve the power of Naive Bayes Model\n",
        "#### Here are some tips for improving power of Naive Bayes Model:\n",
        "\n",
        "* If continuous features do not have normal distribution, we should use transformation or different methods to convert it in normal distribution.\n",
        "* If test data set has zero frequency issue, apply smoothing techniques “Laplace Correction” to predict the class of test data set.\n",
        "* Remove correlated features, as the highly correlated features are voted twice in the model and it can lead to over inflating importance.\n",
        "* Naive Bayes classifiers has limited options for parameter tuning like alpha=1 for smoothing, fit_prior=[True|False] to learn class prior probabilities or not and some other options (look at detail here). I would recommend to focus on your  pre-processing of data and the feature selection.\n",
        "* You might think to apply some classifier combination technique like ensembling, bagging and boosting but these methods would not help. Actually, “ensembling, boosting, bagging” won’t help since their purpose is to reduce variance. Naive Bayes has no variance to minimize.\n",
        "\n",
        "\n",
        "### References\n",
        "* [Introduction to Bayesian Networks with Jhonatan de Souza Oliveira - Machine Learning Mastery](https://machinelearningmastery.com/introduction-to-bayesian-networks-with-jhonatan-de-souza-oliveira/)\n",
        "* [6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R) - Analytics Vidhya](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n",
        "* [Better Naive Bayes: 12 Tips To Get The Most From The Naive Bayes Algorithm - Machine Learning Mastery](https://machinelearningmastery.com/better-naive-bayes/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "185d98e2"
      },
      "source": [
        "**Reasoning**:\n",
        "I will define a Python function `predict_survival` that takes the required features as input, creates a DataFrame, and then uses the pre-trained `classifier` to predict the survival class and its probabilities.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60d59e68"
      },
      "source": [
        "def predict_survival(Age, Embarked, Fare, Parch, Sex, SibSp):\n",
        "    # Create a DataFrame from the input parameters\n",
        "    input_data = pd.DataFrame({\n",
        "        'Age': [Age],\n",
        "        'Embarked': [Embarked],\n",
        "        'Fare': [Fare],\n",
        "        'Parch': [Parch],\n",
        "        'Sex': [Sex],\n",
        "        'SibSp': [SibSp]\n",
        "    })\n",
        "\n",
        "    # Get the survival prediction\n",
        "    prediction = classifier.predict(input_data)[0]\n",
        "\n",
        "    # Get the probabilities of not surviving (0) and surviving (1)\n",
        "    probabilities = classifier.predict_proba(input_data)[0]\n",
        "\n",
        "    return prediction, probabilities\n",
        "\n",
        "print(\"Prediction function 'predict_survival' defined successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0565464c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import `ipywidgets` and `IPython.display`, then create individual widgets for 'Age', 'Embarked', 'Fare', 'Parch', 'Sex', and 'SibSp' as specified, using `IntSlider`, `FloatSlider`, and `Dropdown` types with appropriate ranges, default values, and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a84b618d"
      },
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# 2. Create an IntSlider widget for 'Age'\n",
        "age_widget = widgets.IntSlider(\n",
        "    min=0,\n",
        "    max=9, # 'Age' was binned into 10 categories (0-9)\n",
        "    step=1,\n",
        "    value=4, # Default value\n",
        "    description='Age (Binned):'\n",
        ")\n",
        "\n",
        "# 3. Create a Dropdown widget for 'Embarked'\n",
        "embarked_widget = widgets.Dropdown(\n",
        "    options={'S': 1, 'C': 2, 'Q': 3},\n",
        "    value=1, # Default to 'S'\n",
        "    description='Embarked:'\n",
        ")\n",
        "\n",
        "# 4. Create a FloatSlider widget for 'Fare'\n",
        "fare_widget = widgets.FloatSlider(\n",
        "    min=0.0,\n",
        "    max=512.33,\n",
        "    step=0.1,\n",
        "    value=30.0, # Reasonable default value\n",
        "    description='Fare:'\n",
        ")\n",
        "\n",
        "# 5. Create an IntSlider widget for 'Parch'\n",
        "parch_widget = widgets.IntSlider(\n",
        "    min=0,\n",
        "    max=6, # Based on data description\n",
        "    step=1,\n",
        "    value=0, # Default value\n",
        "    description='Parch:'\n",
        ")\n",
        "\n",
        "# 6. Create a Dropdown widget for 'Sex'\n",
        "sex_widget = widgets.Dropdown(\n",
        "    options={'Female': 0, 'Male': 1},\n",
        "    value=1, # Default to 'Male'\n",
        "    description='Sex:'\n",
        ")\n",
        "\n",
        "# 7. Create an IntSlider widget for 'SibSp'\n",
        "sibsp_widget = widgets.IntSlider(\n",
        "    min=0,\n",
        "    max=8, # Based on data description\n",
        "    step=1,\n",
        "    value=0, # Default value\n",
        "    description='SibSp:'\n",
        ")\n",
        "\n",
        "print(\"Widgets for 'Age', 'Embarked', 'Fare', 'Parch', 'Sex', and 'SibSp' created successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6b0333b"
      },
      "source": [
        "## Build Interactive Dashboard\n",
        "\n",
        "### Subtask:\n",
        "Combine the input widgets and the prediction function into an interactive dashboard. This will include an 'Predict' button to trigger the prediction and display the results dynamically.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "855835f1"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to import the necessary `ipywidgets` classes, create the output widget and button, define the button's click handler, and then arrange and display all the widgets to build the interactive dashboard.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d6df0e6"
      },
      "source": [
        "from IPython.display import display, clear_output\n",
        "\n",
        "# 1. Create an Output widget to display the prediction results\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "# 2. Create a Button widget\n",
        "predict_button = widgets.Button(\n",
        "    description='Predict Survival',\n",
        "    button_style='success', # 'success', 'info', 'warning', 'danger' or ''\n",
        "    tooltip='Click to predict survival'\n",
        ")\n",
        "\n",
        "# 3. Define the function to be executed when the button is clicked\n",
        "def on_predict_button_clicked(button):\n",
        "    with output_widget:\n",
        "        clear_output()\n",
        "        # Get current values from widgets\n",
        "        age_val = age_widget.value\n",
        "        embarked_val = embarked_widget.value\n",
        "        fare_val = fare_widget.value\n",
        "        parch_val = parch_widget.value\n",
        "        sex_val = sex_widget.value\n",
        "        sibsp_val = sibsp_widget.value\n",
        "\n",
        "        # Call the prediction function\n",
        "        prediction, probabilities = predict_survival(\n",
        "            age_val, embarked_val, fare_val, parch_val, sex_val, sibsp_val\n",
        "        )\n",
        "\n",
        "        # Display the results\n",
        "        print(f\"--- Prediction Result ---\")\n",
        "        if prediction == 1:\n",
        "            print(f\"Survival Prediction: SURVIVED (1)\")\n",
        "        else:\n",
        "            print(f\"Survival Prediction: DID NOT SURVIVE (0)\")\n",
        "        print(f\"Probability of Not Surviving (0): {probabilities[0]:.2f}\")\n",
        "        print(f\"Probability of Surviving (1): {probabilities[1]:.2f}\")\n",
        "        print(\"-------------------------\")\n",
        "\n",
        "# 4. Register the on_predict_button_clicked function as the callback for the button's on_click event\n",
        "predict_button.on_click(on_predict_button_clicked)\n",
        "\n",
        "# 5. Arrange the input widgets and the button using VBox and HBox\n",
        "input_widgets_layout = widgets.VBox([\n",
        "    age_widget,\n",
        "    embarked_widget,\n",
        "    fare_widget,\n",
        "    parch_widget,\n",
        "    sex_widget,\n",
        "    sibsp_widget\n",
        "])\n",
        "\n",
        "display(input_widgets_layout, predict_button, output_widget)\n",
        "\n",
        "print(\"Interactive dashboard created successfully. Use the widgets to input values and click 'Predict Survival' to see the results.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b883295"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the interactive front-end created and how to use it to get predictions from the Naive Bayes model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35c0885b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "\n",
        "The interactive front-end is an `ipywidgets` dashboard designed for predicting Titanic survival using a Naive Bayes model.\n",
        "\n",
        "To use it:\n",
        "1.  **Input Features**: Adjust the sliders and dropdown menus for the following passenger features: 'Age (Binned)', 'Embarked', 'Fare', 'Parch' (number of parents/children aboard), 'Sex', and 'SibSp' (number of siblings/spouses aboard).\n",
        "    *   'Age' is represented as a binned integer from 0 to 9.\n",
        "    *   'Embarked' uses a dropdown with 'S', 'C', 'Q' options.\n",
        "    *   'Sex' uses a dropdown with 'Female' and 'Male' options.\n",
        "2.  **Trigger Prediction**: Click the 'Predict Survival' button.\n",
        "3.  **View Results**: The prediction (SURVIVED or DID NOT SURVIVE) and the associated probabilities for both outcomes will be displayed directly below the button.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A `predict_survival` function was successfully implemented, taking six passenger features as input and returning the model's survival prediction (0 or 1) and the probabilities for each outcome.\n",
        "*   Interactive `ipywidgets` were created for all six input features: 'Age' (IntSlider, 0-9), 'Embarked' (Dropdown, S:1, C:2, Q:3), 'Fare' (FloatSlider, 0.0-512.33), 'Parch' (IntSlider, 0-6), 'Sex' (Dropdown, Female:0, Male:1), and 'SibSp' (IntSlider, 0-8).\n",
        "*   An interactive dashboard was successfully assembled, combining the input widgets with a 'Predict Survival' button and an output area.\n",
        "*   The dashboard dynamically displays the survival prediction (e.g., \"SURVIVED (1)\") and the probabilities for both \"Not Surviving (0)\" and \"Surviving (1)\" when the button is clicked.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The interactive dashboard provides a user-friendly interface for exploring the Naive Bayes model's predictions, allowing for immediate feedback on how different passenger attributes might influence survival.\n",
        "*   Next steps could involve integrating this dashboard into a larger application or evaluating the model's performance more rigorously with new data through A/B testing or user studies.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}